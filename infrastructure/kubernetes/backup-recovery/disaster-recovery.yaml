apiVersion: v1
kind: ConfigMap
metadata:
  name: disaster-recovery-config
  namespace: backup-recovery
  labels:
    app: disaster-recovery
    component: data-protection
data:
  dr-procedures.yaml: |
    disaster_recovery:
      rto_target_minutes: 60  # Recovery Time Objective
      rpo_target_minutes: 15  # Recovery Point Objective

      scenarios:
        - name: "primary_region_failure"
          description: "Complete failure of primary AWS region"
          trigger_conditions:
            - "primary_region_health_check_failures > 3"
            - "primary_eks_cluster_unreachable"
          automated_response: true
          steps:
            - "activate_secondary_region"
            - "update_dns_records"
            - "restore_critical_services"
            - "verify_service_health"
            - "notify_stakeholders"

        - name: "database_corruption"
          description: "Critical database corruption or failure"
          trigger_conditions:
            - "database_corruption_detected"
            - "database_connection_failures > 10"
          automated_response: false
          steps:
            - "isolate_corrupted_database"
            - "restore_from_latest_backup"
            - "verify_data_integrity"
            - "restart_dependent_services"

        - name: "security_breach"
          description: "Confirmed security breach requiring isolation"
          trigger_conditions:
            - "security_breach_confirmed"
            - "unauthorized_access_detected"
          automated_response: true
          steps:
            - "isolate_affected_systems"
            - "activate_incident_response"
            - "preserve_forensic_evidence"
            - "restore_from_clean_backup"
            - "implement_additional_security"

  failover-config.yaml: |
    failover:
      primary_region: "us-west-2"
      secondary_region: "us-east-1"

      dns_configuration:
        primary_endpoint: "api.nexafi.com"
        secondary_endpoint: "api-dr.nexafi.com"
        health_check_interval: 30
        failure_threshold: 3

      service_priorities:
        critical:
          - "auth-service"
          - "payment-service"
          - "ledger-service"
          - "api-gateway"
        high:
          - "user-service"
          - "credit-service"
          - "compliance-monitor"
        medium:
          - "analytics-service"
          - "document-service"
          - "ai-service"

      data_replication:
        financial_database:
          method: "streaming_replication"
          lag_threshold_seconds: 30
        user_database:
          method: "logical_replication"
          lag_threshold_seconds: 60
        vault_data:
          method: "raft_snapshot"
          frequency: "hourly"

  monitoring.yaml: |
    health_checks:
      - name: "primary_region_health"
        endpoint: "https://api.nexafi.com/health"
        interval: 30
        timeout: 10
        failure_threshold: 3

      - name: "database_health"
        type: "database_connection"
        connection_string: "postgresql://financial-db.database.svc.cluster.local:5432/financial"
        interval: 60
        timeout: 5
        failure_threshold: 2

      - name: "vault_health"
        endpoint: "https://vault.security.svc.cluster.local:8200/v1/sys/health"
        interval: 60
        timeout: 5
        failure_threshold: 2

    metrics:
      - dr_failover_time_seconds
      - dr_rto_compliance_ratio
      - dr_rpo_compliance_ratio
      - dr_test_success_rate
      - backup_restore_time_seconds

    alerts:
      - name: "DRFailoverTriggered"
        condition: "dr_failover_triggered == 1"
        severity: "critical"
      - name: "RTOViolation"
        condition: "dr_failover_time_seconds > 3600"
        severity: "critical"
      - name: "RPOViolation"
        condition: "backup_age_seconds > 900"
        severity: "critical"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: disaster-recovery-controller
  namespace: backup-recovery
  labels:
    app: disaster-recovery-controller
    component: data-protection
spec:
  replicas: 2
  selector:
    matchLabels:
      app: disaster-recovery-controller
  template:
    metadata:
      labels:
        app: disaster-recovery-controller
        component: data-protection
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: backup-recovery-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: dr-controller
          image: nexafi/disaster-recovery-controller:latest
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
          env:
            - name: PORT
              value: "8080"
            - name: METRICS_PORT
              value: "9090"
            - name: PRIMARY_REGION
              value: "us-west-2"
            - name: SECONDARY_REGION
              value: "us-east-1"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: dr-aws-secret
                  key: access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: dr-aws-secret
                  key: secret_access_key
            - name: ROUTE53_HOSTED_ZONE_ID
              valueFrom:
                secretKeyRef:
                  name: dr-aws-secret
                  key: hosted_zone_id
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: dr-db-secret
                  key: url
            - name: VAULT_ADDR
              value: "https://vault.security.svc.cluster.local:8200"
            - name: VAULT_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vault-token
                  key: token
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: dr-alerts
                  key: slack_webhook
            - name: PAGERDUTY_INTEGRATION_KEY
              valueFrom:
                secretKeyRef:
                  name: dr-alerts
                  key: pagerduty_key
            - name: EMAIL_SMTP_HOST
              value: "smtp.nexafi.com"
            - name: EMAIL_FROM
              value: "dr@nexafi.com"
            - name: AUTOMATED_FAILOVER_ENABLED
              value: "true"
            - name: HEALTH_CHECK_INTERVAL
              value: "30s"
            - name: FAILOVER_THRESHOLD
              value: "3"
            - name: LOG_LEVEL
              value: "INFO"
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9090
              name: metrics
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: config
              mountPath: /app/config
              readOnly: true
            - name: scripts
              mountPath: /app/scripts
              readOnly: true
      volumes:
        - name: tmp
          emptyDir: {}
        - name: config
          configMap:
            name: disaster-recovery-config
        - name: scripts
          configMap:
            name: dr-scripts
            defaultMode: 0755
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - disaster-recovery-controller
                topologyKey: kubernetes.io/hostname
---
apiVersion: v1
kind: Service
metadata:
  name: disaster-recovery-controller
  namespace: backup-recovery
  labels:
    app: disaster-recovery-controller
    component: data-protection
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP
  selector:
    app: disaster-recovery-controller
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dr-scripts
  namespace: backup-recovery
  labels:
    app: disaster-recovery
    component: data-protection
data:
  failover-to-secondary.sh: |
    #!/bin/bash
    set -e

    echo "Starting failover to secondary region..."

    # Update Route53 DNS records
    aws route53 change-resource-record-sets \
      --hosted-zone-id $ROUTE53_HOSTED_ZONE_ID \
      --change-batch file:///app/scripts/dns-failover.json

    # Scale up secondary region EKS cluster
    aws eks update-nodegroup-config \
      --cluster-name nexafi-secondary \
      --nodegroup-name financial-services \
      --scaling-config minSize=3,maxSize=10,desiredSize=5 \
      --region $SECONDARY_REGION

    # Deploy critical services to secondary region
    kubectl apply -f /app/manifests/critical-services/ \
      --context=secondary-cluster

    # Wait for services to be ready
    kubectl wait --for=condition=ready pod \
      -l tier=critical \
      --timeout=300s \
      --context=secondary-cluster

    # Verify service health
    /app/scripts/verify-services.sh secondary

    echo "Failover to secondary region completed"

  failback-to-primary.sh: |
    #!/bin/bash
    set -e

    echo "Starting failback to primary region..."

    # Ensure primary region is healthy
    /app/scripts/verify-services.sh primary

    # Sync data from secondary to primary
    /app/scripts/sync-data.sh secondary primary

    # Deploy services to primary region
    kubectl apply -f /app/manifests/all-services/ \
      --context=primary-cluster

    # Wait for services to be ready
    kubectl wait --for=condition=ready pod \
      -l tier=critical \
      --timeout=300s \
      --context=primary-cluster

    # Update Route53 DNS records back to primary
    aws route53 change-resource-record-sets \
      --hosted-zone-id $ROUTE53_HOSTED_ZONE_ID \
      --change-batch file:///app/scripts/dns-failback.json

    # Scale down secondary region
    aws eks update-nodegroup-config \
      --cluster-name nexafi-secondary \
      --nodegroup-name financial-services \
      --scaling-config minSize=1,maxSize=3,desiredSize=1 \
      --region $SECONDARY_REGION

    echo "Failback to primary region completed"

  verify-services.sh: |
    #!/bin/bash
    set -e

    REGION=$1
    CONTEXT="${REGION}-cluster"

    echo "Verifying services in $REGION region..."

    # Check critical services
    CRITICAL_SERVICES=("auth-service" "payment-service" "ledger-service" "api-gateway")

    for service in "${CRITICAL_SERVICES[@]}"; do
      echo "Checking $service..."

      # Check if pods are running
      kubectl get pods -l app=$service \
        --context=$CONTEXT \
        --field-selector=status.phase=Running

      # Check service health endpoint
      SERVICE_IP=$(kubectl get svc $service \
        --context=$CONTEXT \
        -o jsonpath='{.spec.clusterIP}')

      curl -f http://$SERVICE_IP:8080/health || {
        echo "Health check failed for $service"
        exit 1
      }
    done

    echo "All critical services are healthy in $REGION region"

  sync-data.sh: |
    #!/bin/bash
    set -e

    SOURCE_REGION=$1
    TARGET_REGION=$2

    echo "Syncing data from $SOURCE_REGION to $TARGET_REGION..."

    # Sync financial database
    pg_dump -h financial-db-$SOURCE_REGION.nexafi.com \
      -U postgres financial | \
    psql -h financial-db-$TARGET_REGION.nexafi.com \
      -U postgres financial

    # Sync user database
    pg_dump -h user-db-$SOURCE_REGION.nexafi.com \
      -U postgres users | \
    psql -h user-db-$TARGET_REGION.nexafi.com \
      -U postgres users

    # Sync Vault data
    vault operator raft snapshot save \
      -address=https://vault-$SOURCE_REGION.nexafi.com:8200 \
      /tmp/vault-snapshot.snap

    vault operator raft snapshot restore \
      -address=https://vault-$TARGET_REGION.nexafi.com:8200 \
      /tmp/vault-snapshot.snap

    echo "Data sync completed from $SOURCE_REGION to $TARGET_REGION"

  dns-failover.json: |
    {
      "Changes": [
        {
          "Action": "UPSERT",
          "ResourceRecordSet": {
            "Name": "api.nexafi.com",
            "Type": "A",
            "SetIdentifier": "primary",
            "Failover": {
              "Type": "SECONDARY"
            },
            "TTL": 60,
            "ResourceRecords": [
              {
                "Value": "52.12.34.56"
              }
            ]
          }
        }
      ]
    }

  dns-failback.json: |
    {
      "Changes": [
        {
          "Action": "UPSERT",
          "ResourceRecordSet": {
            "Name": "api.nexafi.com",
            "Type": "A",
            "SetIdentifier": "primary",
            "Failover": {
              "Type": "PRIMARY"
            },
            "TTL": 60,
            "ResourceRecords": [
              {
                "Value": "34.56.78.90"
              }
            ]
          }
        }
      ]
    }
---
# DR Test CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test
  namespace: backup-recovery
  labels:
    app: dr-test
    component: data-protection
spec:
  schedule: "0 2 * * 0" # Weekly on Sunday at 2 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: dr-test
            component: data-protection
        spec:
          serviceAccountName: backup-recovery-sa
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
          containers:
            - name: dr-tester
              image: nexafi/dr-tester:latest
              imagePullPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                readOnlyRootFilesystem: true
                capabilities:
                  drop:
                    - ALL
              env:
                - name: TEST_TYPE
                  value: "automated"
                - name: SECONDARY_REGION
                  value: "us-east-1"
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: dr-aws-secret
                      key: access_key_id
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: dr-aws-secret
                      key: secret_access_key
                - name: SLACK_WEBHOOK_URL
                  valueFrom:
                    secretKeyRef:
                      name: dr-alerts
                      key: slack_webhook
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  echo "Starting DR test..."

                  # Test backup restoration
                  /app/scripts/test-backup-restore.sh

                  # Test secondary region readiness
                  /app/scripts/test-secondary-region.sh

                  # Test failover procedures (simulation)
                  /app/scripts/test-failover-simulation.sh

                  # Generate test report
                  /app/scripts/generate-dr-report.sh

                  echo "DR test completed successfully"
              volumeMounts:
                - name: tmp
                  mountPath: /tmp
                - name: test-scripts
                  mountPath: /app/scripts
                  readOnly: true
              resources:
                requests:
                  memory: "256Mi"
                  cpu: "250m"
                limits:
                  memory: "512Mi"
                  cpu: "500m"
          volumes:
            - name: tmp
              emptyDir: {}
            - name: test-scripts
              configMap:
                name: dr-scripts
                defaultMode: 0755
          restartPolicy: OnFailure
